<!DOCTYPE html>



  

<html class="theme-next pisces use-motion" lang="">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />





  <link rel="alternate" href="/atom.xml" title="大毛毛啊" type="application/atom+xml" />






<meta property="og:type" content="website">
<meta property="og:title" content="大毛毛啊">
<meta property="og:url" content="http://www.xuman-amy.cn/index.html">
<meta property="og:site_name" content="大毛毛啊">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="大毛毛啊">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"right","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://www.xuman-amy.cn/"/>





  <title>大毛毛啊</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">
<a href="https://github.com/xuman-Amy"><img style="position: absolute; top: 0; left: 0; border: 0;" src="https://s3.amazonaws.com/github/ribbons/forkme_left_darkblue_121621.png" alt="Fork me on GitHub"></a>

  
  
    
  

  <div class="container sidebar-position-right 
  page-home">
    <div class="headband">
    <a href="https://github.com/xuman-Amy"><img style="position: absolute; top: 0; left: 0; border: 0;" src="https://s3.amazonaws.com/github/ribbons/forkme_left_darkblue_121621.png" alt="Fork me on GitHub"></a>
    </div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">大毛毛啊</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.xuman-amy.cn/2018/09/25/算法导论/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Amy">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="大毛毛啊">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/09/25/算法导论/" itemprop="url">算法导论</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-09-25T22:23:20+08:00">
                2018-09-25
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/book/" itemprop="url" rel="index">
                    <span itemprop="name">book</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          
             <span id="/2018/09/25/算法导论/" class="leancloud_visitors" data-flag-title="算法导论">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">Visitors&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script></p>
<p>&lt;script type=”text/javascript”src=”<a href="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot;&gt;" target="_blank" rel="noopener">http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot;&gt;</a><br><strong>开始看《算是法导论》一书，希望能够坚持看书和题</strong></p>
<h1 id="第一章算法在计算中的应用"><a href="#第一章算法在计算中的应用" class="headerlink" title="第一章算法在计算中的应用"></a>第一章算法在计算中的应用</h1><p>印象最深，最有体会的是“作为一种技术的算法”。<br>作者利用插入排序和归并排序的时间复杂度举例说明了算法作为一种技术的重要性。</p>
<p>插入排序时间复杂度：<code>$c_{1}n^{2}$</code></p>
<p>归并排序时间复杂度：<code>$c_{2} n log_{2}^{n}$</code></p>
<p>一般来说，插入排序的常数项要小于归并排序的常数项，即<code>$c_{1} &lt;c_{2}$</code></p>
<p>很明显二者的不同之处在于n与logn的区别，当数量较小时，插入排序时间较少，但是当达到一定的数量级后，归并排序的优越性显而易见了。</p>
<p>举个例子：</p>
<p>假设有一个1000万的数组，如果每个数是8字节的整数，那么也就是80M(（<code>$8 * 10^{7}$</code>)的数据。</p>
<p>假设插入排序采用性能较优的硬件A+顶配的程序猿，即A电脑美妙执行百亿（<code>$10^{11}$</code>）条指令假设常数项为2，那么共<code>$2 n^{2}$</code>条指令</p>
<p>归并排序采用比A慢1000倍的硬件B，每秒执行1000万(<code>$10^{8}$</code>)条，假设遇到一个菜鸟程序猿，常数项为50，那么共<code>$ 50 n lg{n}$</code>条指令。</p>
<p>所以，插入排序所需时间为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Insert:  \frac&#123;2 \cdot (8*10^&#123;7&#125;)^&#123;2&#125;&#125;&#123;10 * 10^&#123;10&#125;&#125; = 20000(s) = 5.5h</span><br><span class="line"></span><br><span class="line">Merge:    \frac&#123;50 \cdot (8*10^&#123;7&#125;) lg (8*10^&#123;7&#125;)&#125; &#123;10 * 10^&#123;7&#125;&#125; = 1163(s) = 20min</span><br></pre></td></tr></table></figure>
<h2 id="震惊吧！！"><a href="#震惊吧！！" class="headerlink" title="震惊吧！！"></a>震惊吧！！</h2>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.xuman-amy.cn/2018/09/21/createte-blog-with-hexo/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Amy">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="大毛毛啊">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/09/21/createte-blog-with-hexo/" itemprop="url">createte_blog_with_hexo</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-09-21T17:44:19+08:00">
                2018-09-21
              </time>
            

            

            
          </span>

          

          
            
          

          
          
             <span id="/2018/09/21/createte-blog-with-hexo/" class="leancloud_visitors" data-flag-title="createte_blog_with_hexo">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">Visitors&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="使用hexo和github搭建个人博客"><a href="#使用hexo和github搭建个人博客" class="headerlink" title="使用hexo和github搭建个人博客"></a>使用hexo和github搭建个人博客</h1><p>欢迎访问个人博客~          <a href="http://www.xuman-amy.cn/">大毛毛啊</a></p>
<ol>
<li>安装node.js并配置环境</li>
</ol>
<p><img src="https://note.youdao.com/yws/api/personal/file/EA80E525262640189AC7B6234B82EC48?method=download&amp;shareKey=766bfede1819f98a5899efa0b66f4dba" alt=""></p>
<ol start="2">
<li>安装git</li>
</ol>
<p>进入github官网，新建仓库。</p>
<p><img src="https://note.youdao.com/yws/api/personal/file/435FE85F9FDA48A6864AB5CD3534ECE3?method=download&amp;shareKey=502b98616b2a903c9ced1cff860902e4" alt=""></p>
<ol start="3">
<li>安装hexo</li>
</ol>
<p>打开cmd终端，进入你想安装的目录，输入命令</p>
<figure class="highlight plain"><figcaption><span>install hexo -g```</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">验证安装是否成功</span><br><span class="line"></span><br><span class="line">![](https://note.youdao.com/yws/api/personal/file/8D5B87A027CA499EB1C4C66934884B86?method=download&amp;shareKey=91590e39b455e7026feb645709f13247)</span><br><span class="line"></span><br><span class="line">4. 初始化hexo</span><br><span class="line"></span><br><span class="line">进入一个空目录，输入命令   </span><br><span class="line">```hexo init</span><br></pre></td></tr></table></figure>
<p><strong>喝杯豆浆，静静等待~~~嘻嘻</strong></p>
<p>安装完成，看到了 “INFO  Start blogging with Hexo!”</p>
<ol start="5">
<li>安装hexo所需组件</li>
</ol>
<figure class="highlight plain"><figcaption><span>install```</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">6. 测试一下hexo，主题等可以后期更改</span><br><span class="line"></span><br><span class="line">```hexo -g```构建blog  </span><br><span class="line">```hexo -s```发布在本地服务器  </span><br><span class="line"></span><br><span class="line">7. 连接git</span><br><span class="line"></span><br><span class="line">进入目录_config.yml,配置最后的deploy，注意格式，冒号后个空格，每行开始空两个空格。</span><br></pre></td></tr></table></figure>
<p>deploy:<br>  type: git<br>  repository: <a href="mailto:git@github.com" target="_blank" rel="noopener">git@github.com</a>:xuman-Amy/xuman-Amy.github.io.git<br>  branch: master<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">之后下载一个插件</span><br><span class="line"></span><br><span class="line">```npm install hexo-deployer-git --save</span><br></pre></td></tr></table></figure></p>
<ol start="8">
<li>测试一下新博文</li>
</ol>
<figure class="highlight plain"><figcaption><span>new "blog_name" ```  </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">```hexo g```  </span><br><span class="line">```hexo d</span><br></pre></td></tr></table></figure>
<p>关于优化的小故事，有时间会继续更的~~</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.xuman-amy.cn/2018/09/21/hello-world/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Amy">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="大毛毛啊">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/09/21/hello-world/" itemprop="url">Hello World</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-09-21T17:28:03+08:00">
                2018-09-21
              </time>
            

            

            
          </span>

          

          
            
          

          
          
             <span id="/2018/09/21/hello-world/" class="leancloud_visitors" data-flag-title="Hello World">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">Visitors&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.xuman-amy.cn/2018/08/23/Chinese-NER-Lattice-lstm/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Amy">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="大毛毛啊">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/08/23/Chinese-NER-Lattice-lstm/" itemprop="url">Chinese-NER-Lattice-lstm</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-08-23T10:49:30+08:00">
                2018-08-23
              </time>
            

            

            
          </span>

          

          
            
          

          
          
             <span id="/2018/08/23/Chinese-NER-Lattice-lstm/" class="leancloud_visitors" data-flag-title="Chinese-NER-Lattice-lstm">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">Visitors&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script></p>
<h1 id="paper-title：《Chinese-NER-Using-Lattice-LSTM》"><a href="#paper-title：《Chinese-NER-Using-Lattice-LSTM》" class="headerlink" title="paper title：《Chinese NER Using Lattice LSTM》"></a>paper title：《Chinese NER Using Lattice LSTM》</h1><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>对于中文命名实体识别，我们研究了一种lattice结构的lstm模型（lattice-structured LSTM），这个模型能够对输入的字符序列以及序列中能够与字典相匹配的隐藏的词信息一同编码。</p>
<p>相比于基于字粒度的方法，lattice的能够更好的利用字以及字序列的信息。相比于词粒度的方法，lattice避免了分词带来的错误。门控循环单元能够使得模型选择最相关的字和词，得到更好的NER结果。在多种数据的实验中表明，lattice的性能优于基于字粒度和词粒度的lstm，能够得到best结果。</p>
<h1 id="1-引言"><a href="#1-引言" class="headerlink" title="1 引言"></a>1 引言</h1><p>近几年，作为信息抽取领域中的基本任务，命名实体识别（NER）得到了持续的研究关注。NER一直被看作是一项序列标注问题，他的实体边缘和类别标记一起被预测出来。英文NER中最好的结果是通过lstm-crf模型将字信息融入到词表示中得到的。</p>
<p>中文NER与分词有关，特别是实体的边界就是词的边界。中文NER的一种方式是先分词，然后对词序列进行NER标注。这种分词-&gt;NER的pipeline很容易受到传播错误，因为在分词中命名实体是OOV（out-of-vocabulary）的重要来源，而且错误的分割实体边缘也会导致NER的错误。这种问题在开放领域更为严重，因为交叉领域中的词分割还存在很多待解决问题。所以词粒度的性能要比字粒度的性能要好。</p>
<p>基于字粒度的NER的缺点是忽略了字以及字序列的信息。解决方法是利用lattice结构的lstm将句子中潜在的词信息融入到字粒度的lstm-crf中。</p>
<p><img src="https://note.youdao.com/yws/api/personal/file/WEBbaf4bde4736a74b5ab54f2b1656e16c1?method=download&amp;shareKey=0478b2c82daa1e4a30d679217098d62e" alt="figure 1 "></p>
<p>如上图，通过将句子与自动得到的较大的词典匹配构建一个字-词的lattice结构。例子中是得到了“长江大桥”“大桥”“长江”可以消除文中潜在命名实体的歧义，例如与文中的“江大桥”。</p>
<p>因为一个lattice中的字-词会有指数级的路径，所以构建了一个lattice lstm结构自动控制信息流从句子开始流到句子结尾。</p>
<p><img src="https://note.youdao.com/yws/api/personal/file/WEBf49ef09e4a09dcb297d4025d15f14123?method=download&amp;shareKey=f22f298a052037ebae73925e985c7fbd" alt="figure 2"></p>
<p>如上图，通过门单元动态的将不同路径得到的词输送到相应的字符。在NER数据上训练，lattice lstm能够自动的在文中找到更多的词信息来达到更好的NER性能。与只依赖于字信息和只只依赖于词信息的NER相比，lsttice lstm 能够不受分词错误的影响，并将明确的词信息作用于字序列标注上的优点。</p>
<p><strong><a href="https://github.com/jiesutd/LatticeLSTM" target="_blank" rel="noopener">代码及数据git地址</a></strong></p>
<h1 id="2-相关工作"><a href="#2-相关工作" class="headerlink" title="2 相关工作"></a>2 相关工作</h1><p>神经网络运用与NER的方法：</p>
<ul>
<li>Hammerton ：单向lstm，第一次将神经网络应用于NER</li>
<li>Collobert ：CNN-CRF</li>
<li>dos Santos：character CNN + cnn-crf</li>
<li>最多的：lstm-crf</li>
<li>Lample：character LSTM</li>
</ul>
<p>基于字序列的标注成为中文NER的主流方法。在统计上来看，对于字粒度和词粒度的NER<br>方法的讨论表明字粒度的性能更优，对于神经网络中的NER方法也是如此。<br>另外，lattice lstm要优于word LSTM和character LSTM.</p>
<p>本模型不需要分词也不需要考虑多任务的设置。</p>
<p>NER任务中会经常使用外部信息源，尤其是词典特征被广泛使用。在本论文中是在大量自动分词的文本中使用预训练的word embedding 词典。同时类似与语言模型的半监督方法也可以到lattice lstm中。</p>
<p>lattice 结构的RNN可以看作是树结构的RNN向DAGs(Directed acyclic graph，有向无环图）的延伸。本文中的lattice lstm是以字符为中心的lattice-lstm-crf序列标注模型，对于词，有循环单元但是没有隐藏单元。</p>
<p>是第一个将字符与词混合起来构建lattice的，也是都一个将word-character lattice 应用于不用分词的中文NER任务中的。</p>
<h1 id="3-模型"><a href="#3-模型" class="headerlink" title="3 模型"></a>3 模型</h1><p>使用lstm-crf作为主要的网络结构。</p>
<p>一般将输入序列表示为<code>$s = c_{1},c_{2}...c_{m}$</code><br>其中，<code>$c_{j}$</code>代表第j个字符。也可以把输入表示为<code>$s = w_{1},w_{2}...w_{m}$</code>，其中<code>$w_{j}$</code>表示句子分词后的第j个词。本文中应用t(i,k)表示索引j，代表第i个单词的第k个字符。比如“南京市 长江大桥”,索引从1开始，那么t(1,2) = 2(京)，t(2,3) = 6（大）。<br>运用BIOES（begin intermediate other end single）标注策略进行字粒度和词粒度的NER标注。</p>
<h2 id="3-1-Character-Based-Model"><a href="#3-1-Character-Based-Model" class="headerlink" title="3.1 Character-Based Model"></a>3.1 Character-Based Model</h2><p><img src="https://note.youdao.com/yws/api/personal/file/WEB7f49214d676576dcdda9aad470b0db33?method=download&amp;shareKey=8e11349817c7066b339a9bd9595564ce" alt="char-leverl"></p>
<p>字粒度的ner模型如上图所示。<br>运用lstm-crf模型计算字序列<code>$c_{1},c_{2} ....c_{m}$</code>，每个字符<code>$c_{j}$</code>通过字向量查找表（character embedding lookup table）得到。<br>$$<br>X_{j}^{c} = e^{c}(c_{j}) \qquad(1)<br>$$<br>双向lstm计算字向量<code>$x_{1},x_{2} ...x_{m}$</code>得到隐藏层结果<code>$\overrightarrow(h_{1}^{c}),\overrightarrow(h_{2}^{c}) ...\overrightarrow(h_{m}^{c})$</code>以及反向的<code>$\overleftarrow(h_{1}^{c}),\overleftarrow(h_{2}^{c}) ...\overleftarrow(h_{m}^{c})$</code>，最后得到隐藏层结果为</p>
<p>$$<br>h_{c}^{j} = [\overrightarrow(h^{c}<em>{j});\overleftarrow(h^{c}</em>{j}] \qquad (2)<br>$$</p>
<p>最后应用CRF（公式（17））计算隐藏层结果<code>$h_{c}^{j}$</code>.</p>
<ul>
<li><p>Char + bichar. </p>
<p>实验表明Character bigrams对于词分割有很大用处。通过拼接双字的embedding和单字的embedding，将字符 bigrams添加到字粒度的模型中。</p>
</li>
</ul>
<p>$$<br> X_{j}^{c} = [e^{c}(c_{j});e^{b}(c_{j},c_{j+1})] \qquad (3)<br>$$</p>
<ul>
<li>Char + softword<br>将分词作为soft 特征能够提升性能，通过将分词的embedding与字符embedding拼接起来，将分词信息加到 character representation 中。</li>
</ul>
<p>$$<br> X_{j}^{c} = [e^{c}(c_{j});e^{s}( seg(c_{j}) )] \qquad (4)<br>$$</p>
<p>得到的隐藏层状态如下：</p>
<p>$$<br>h_{i}^{w} = [\overrightarrow(h^{w}<em>{i});\overleftarrow(h^{w}</em>{i}] \qquad (5)<br>$$</p>
<h2 id="3-2-Word-Based-Model"><a href="#3-2-Word-Based-Model" class="headerlink" title="3.2 Word-Based Model"></a>3.2 Word-Based Model</h2><p><img src="https://note.youdao.com/yws/api/personal/file/WEB6290f32a0f17abc4adcb550261f48fbd?method=download&amp;shareKey=628adda913259538922e1a037be7cdea" alt="word-level"></p>
<p>基于词粒度的NER如上。利用word embedding得到词的向量表示。</p>
<p>$$<br>X_{i}^{w} = e^{w}(w_{i}) \qquad(6)<br>$$</p>
<p>同样运用双向lstm得到双向隐藏层状态序列，并拼接得到最终的隐藏层状态。</p>
<p>$$<br>h_{i}^{w} = [\overrightarrow(h^{w}<em>{i});\overleftarrow(h^{w}</em>{i}] \qquad<br>$$</p>
<p>Integrating character representations:</p>
<p>字符CNN和LSTM中都运用词来表示字符。对于本文的中文NER，两者都用上。也就是说词的表示为word embedding查找的词向量和字符序列拼接起来。</p>
<p>$$<br>X^{w}<em>{i} = [e^{w}</em>{i}(w_{i}); X^{c}_{i}] \qquad (7)<br>$$</p>
<ul>
<li>Word + char LSTM</li>
</ul>
<p>每个输入字符的embedding表示为<code>$e^{c}(c_{j})$</code> ,对于<code>$w_{i}$</code>的字符<code>$c_{}$</code> bi-lstm计算字向量得到<code>$\overrightarrow(h_{t(i,1)}^{c}),\overrightarrow(h_{t(i,2)}^{c}) ...\overrightarrow(h_{t(i,len(i))}^{c})$</code>，以及反向的<code>$\overrightarrow(h_{t(i,1)}^{c}),\overrightarrow(h_{t(i,2)}^{c}) ...\overrightarrow(h_{t(i,len(i))}^{c})$</code>,最终对于<code>$w_{i}$</code>的字符级的表示为</p>
<p>$$<br>X^{c}<em>{i} = [\overrightarrow(h</em>{t(i,len(i))}^{c});\overrightarrow(h_{t(i,len(i))}^{c})]<br>$$</p>
<ul>
<li>Word + char LSTM′</li>
</ul>
<ul>
<li>Word + char CNN</li>
</ul>
<p>对于每个单词的字符序列应用标准的CNN，得到字符表示<code>$X^{c}_{i}$</code></p>
<p>$$</p>
<p>X^{c}<em>{i} = max</em>{t(i,1) &lt;= j &lt;= t(i,len(i))}<br>(W_{CNN}^{T}<br>\begin{bmatrix}e^{c}(c_{j - \frac {ke-1} {2})} \<br> … \<br>e^{c}(c_{j + \frac {ke-1} {2})}<br>\end{bmatrix}</p>
<ul>
<li>b_{CNN})<br>$$</li>
</ul>
<p><code>$W_{CNN} \quad b_{CNN}$</code>是参数，ke代表kenal个数，max表示最大pooling。</p>
<h2 id="3-3-Lattice-Model"><a href="#3-3-Lattice-Model" class="headerlink" title="3.3 Lattice Model"></a>3.3 Lattice Model</h2><p>lattice lstm 的整体结构如上图figure2 所示，可以看作是char-based，加上了word-based cells和门控单元控制信息流向。</p>
<p><img src="https://note.youdao.com/yws/api/personal/file/WEB66f6c5e6c56c887a97df9203f2a66720?method=download&amp;shareKey=59cbc3665400a0a54f1497087c4c3577" alt="lattice_ner"></p>
<p>如上图所示，输入数据是字符序列 <code>$ c_{1}..c_{m}$</code>以及与词典匹配的子序列（词）。</p>
<p><code>$W_{b,e}^{d}$</code>代表由索引b开始到索引e结束的子序列，比如“南京市长江大桥”<code>$W^{d}_{1,2}$</code> 即“南京”，<code>$W^{d}_{6,7}$</code> 即“大桥”。</p>
<p>在lattice模型中有四种向量，input vectors, output hidden vectors, cell vectors ，gate vectors。</p>
<p>作为基本组成，在character-based模型中，char输入向量表示每个字符。<br><code>$X_{j}^{c} = e^{c}(c_{j})$ \qquad (10)</code></p>
<p>基本的循环结构组成为：字符细胞向量(character cell vector)<code>$c^{c}_{j}$</code>以及对于每个<code>$c_{j}$</code>得到的隐藏层向量<code>$h_{j}^{c}$</code>,其中<code>$c^{c}_{j}$</code>表示从句子开始到<code>$c_{j}$</code>的信息流，<code>$h^{c}_{j}$</code>用于CRF进行序列标记。</p>
<p>基本LSTM函数如下：</p>
<p><img src="https://note.youdao.com/yws/api/personal/file/WEBf8b64dedd312ff26aeaab0d9d9f38d3d?method=download&amp;shareKey=3491775a5181e4888dae9658b272c0ac" alt="lstm function"></p>
<p><strong>lattice lstm</strong><br>不同于char-based lstm，<code>$c^{c}_{j}$</code>的组成要考虑句子中与词典相匹配的子序列<code>$w_{b,e}^{d}$</code>.每个子序列可以表示为</p>
<p>$$<br>X_{b,e}^{w} = e^{w}(w_{b,e}^{d}) \qquad (12)<br>$$</p>
<p>另外，字细胞（word cell ）<code>$c_{b,e}^{w}$</code>表示从句子开始到该位置<code>$X_{b,e}^{w}$</code>的细胞状态。<code>$c_{b,e}^{w}$</code>的计算公式如下：</p>
<p><img src="https://note.youdao.com/yws/api/personal/file/WEB063b9d81753b5c3e9ae9082dc3cab689?method=download&amp;shareKey=28ba20b3cb2149916b83ae3d05556574" alt="lattice word cell"></p>
<p>没有输出，因为只有字粒度才有标注输出。</p>
<p>对于<code>$c_{b,e}^{w}$</code>，有多种路径使得信息流向）<code>$c_{j}^{c}$</code>。比如“南京市长江大桥”中，<code>$c_{7}^{c}$</code>包括<br><code>$X_{7}^{c}$</code>（桥），<br><code>$c_{6,7}^{w}$</code>（大桥）以及<br><code>$c_{4,7}^{w}$</code>（长江大桥）。<br>将所有由<code>$ b \in \{b&#39;| w_{b&#39;,e}^{d} \in D\}$</code>开始的的<code>$c_{b,e}^{w}$</code>与<code>$c_{e}^{c}$</code>联系起来。用另一个门<code>$i_{b,e}^{c}$</code> 来控制信息<code>$c_{b,e}^{w}$</code>流向<code>$c_{b,e}^{c}$</code>。</p>
<p>$$<br>i^{c}<em>{b,e} = \sigma (W^{lT}<br>\begin{bmatrix}<br>X</em>{e}{c} \<br>c_{b,e}^{w}<br>\end{bmatrix}<br>+b^{l}<br>\qquad (14)<br>$$</p>
<p>所以，细胞状态<code>$c_{j}^{c}$</code>的计算变为：</p>
<p>$$<br>c_{j}^{c} =  \sum_{b\in {b’| w_{b’,j}^{d} \in D}}<br>\alpha_{b,j}^{c} \bigodot  c_{b,j}^{w} +<br>\alpha^{c}<em>{j} \bigodot \tilde(c</em>{j}^{c}) \qquad (15)<br>$$</p>
<p><img src="https://note.youdao.com/yws/api/personal/file/WEB4fdfcede7d46963b0ed30bb105748a7f?method=download&amp;shareKey=8b67edfa873a201e290276a95e4c1012" alt=""></p>
<p>最终的隐藏状态<code>$h_{j}^{c}$</code>计算公式与Eq（11）相同。</p>
<p>$$<br>h_{j}^{c} = o_{j}^{c} \bigodot tanh(c_{j}^{c})<br>$$</p>
<p>通过损失函数的最优化，反向传播调整<code>$W^{c},b^{c},W^{w},b^{w},W^{l},b^{l}$</code>使得模型在NER标记过程中能够动态选择相关性最强的词。</p>
<h2 id="3-4-Decoding-and-Training"><a href="#3-4-Decoding-and-Training" class="headerlink" title="3.4 Decoding and Training"></a>3.4 Decoding and Training</h2><p>CRF层在隐藏层的上一层，标记序列y的概率计算公式为：</p>
<p><img src="https://note.youdao.com/yws/api/personal/file/WEBa7b3186a364025dbbba392bf31824e26?method=download&amp;shareKey=7ea8071bcd530044d91adbabf9f3d8af" alt="eq 17"></p>
<p>在word-based或者char-based输入序列上运用一阶维特比算法得到得分最高的标记序列。<br>给定手工标注的训练数据<code>$\{(s_{i}, y_{i})\} | _{i=1}^{N}$</code>,句子级别的对数似然损失函数（使用L2正则）计算公式为：</p>
<p><img src="https://note.youdao.com/yws/api/personal/file/WEBc713500f216293eb80ddf385e291f2a9?method=download&amp;shareKey=74b15993625a05007024583c2b743771" alt="loss function"></p>
<h1 id="4-Experimental"><a href="#4-Experimental" class="headerlink" title="4 Experimental"></a>4 Experimental</h1><p>展开了一系列实验拓展研究词粒度的lattice lstm 在NER上的性能。另一个目的是对比不同参数设置下的基于字和基于词的中文NER性能。</p>
<h2 id="4-1-Experimental"><a href="#4-1-Experimental" class="headerlink" title="4.1  Experimental"></a>4.1  Experimental</h2><ul>
<li>数据：</li>
</ul>
<p><img src="https://note.youdao.com/yws/api/personal/file/WEB591a50b93830ceace48efaec89918c99?method=download&amp;shareKey=95d02799e39a53cae9a0f8c4b042a2f1" alt="data"></p>
<p>resume数据开放。</p>
<ul>
<li><p>分词：</p>
<p>OntoNotes 以及 MSRA train data ：固定标准的分割。<br>Yang 神经网络分词应用于MSRA的test/dev的分词。</p>
</li>
</ul>
<ul>
<li>Word Embeddings</li>
</ul>
<p>使用word2vec在 自动分词的Chinese Giga-Word数据集上 预训练了embedding。Character and character bigram embeddings也是基于此预训练的。</p>
<ul>
<li>Hyper-parameter settings. </li>
</ul>
<p><img src="https://note.youdao.com/yws/api/personal/file/WEB64fc7b64f4d13721916ac6eabf280291?method=download&amp;shareKey=6a6e2b9c6acb33d2b7d17f714f80fd51" alt="Hyper-parameter"></p>
<p>在word embedding和char embedding中都使用dropout层，选择SGD进行损失函数的优化。</p>
<h2 id="4-2-Development-Experiments"><a href="#4-2-Development-Experiments" class="headerlink" title="4.2 Development Experiments"></a>4.2 Development Experiments</h2><ul>
<li><p>Character-based NER</p>
</li>
<li><p>Word-based NER.</p>
</li>
</ul>
<p><img src="https://note.youdao.com/yws/api/personal/file/WEB8218451fc2f6fdc9e8a0fc37edc7f920?method=download&amp;shareKey=1e0d01638c9cddefcf70d2e34cc123cd" alt="Character-Word-based result"></p>
<ul>
<li>Lattice-based NER. </li>
</ul>
<p><img src="https://note.youdao.com/yws/api/personal/file/WEB7ea3e529044a35661d32c8266dd9f562?method=download&amp;shareKey=8870a7fbfd66e3b7665269e5aa641957" alt=""></p>
<h2 id="4-3-Final-Results"><a href="#4-3-Final-Results" class="headerlink" title="4.3 Final Results"></a>4.3 Final Results</h2><p><img src="https://note.youdao.com/yws/api/personal/file/WEBb25cef51add1bc41ce58f11ab6481eef?method=download&amp;shareKey=ccb230ab17b94679a58a10595c3a5686" alt="final res"></p>
<h2 id="4-4-Discussion"><a href="#4-4-Discussion" class="headerlink" title="4.4 Discussion"></a>4.4 Discussion</h2><ul>
<li>F1 against sentence length.</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.xuman-amy.cn/2018/08/23/test/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Amy">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="大毛毛啊">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/08/23/test/" itemprop="url">test</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-08-23T10:33:01+08:00">
                2018-08-23
              </time>
            

            

            
          </span>

          

          
            
          

          
          
             <span id="/2018/08/23/test/" class="leancloud_visitors" data-flag-title="test">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">Visitors&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<p>$$<br>ddddd_{sss}<br>$$</p>
<p><code>$dddd_{ss}$</code></p>
<p>$ff_{ss}$</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.xuman-amy.cn/2018/08/13/Neural-Architectures-for-Named-Entity-Recognition/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Amy">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="大毛毛啊">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/08/13/Neural-Architectures-for-Named-Entity-Recognition/" itemprop="url">《Neural Architectures for Named Entity Recognition》 paper notes</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-08-13T00:00:00+08:00">
                2018-08-13
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NER/" itemprop="url" rel="index">
                    <span itemprop="name">NER</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          
             <span id="/2018/08/13/Neural-Architectures-for-Named-Entity-Recognition/" class="leancloud_visitors" data-flag-title="《Neural Architectures for Named Entity Recognition》 paper notes">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">Visitors&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="《Neural-Architectures-for-Named-Entity-Recognition》"><a href="#《Neural-Architectures-for-Named-Entity-Recognition》" class="headerlink" title="《Neural Architectures for Named Entity Recognition》"></a>《Neural Architectures for Named Entity Recognition》</h1><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>为了从有监督的比较小的训练词典中有效学习，最好的命名实体识别系统（named entity recognition）的实现很大部分依赖于手工标注的特征（features）和领域知识（domain-specific knowledge）。</p>
<p>本论文介绍两个新的网络架构，一个是基于双向lstm和条件岁机场，另一个是受shift-reduce解析器的启发，基于转换的方法进行构建和标注片段。</p>
<p>本文模型依赖于词信息的两个信息源，一个是来源于有监督词典的基于字符的单词表示；另一个是基于无监督的未标注的词表示。</p>
<p>在没有借助任何语言相关的知识和类似地名索引（gazetteers）资源的前提下，我们的模型在四种语言的NER中得到了最好的性能。</p>
<h1 id="1-引言"><a href="#1-引言" class="headerlink" title="1 引言"></a>1 引言</h1><p>NER是一个很有挑战性的学习问题。一方面因为在大多数的语言和领域中，可利用的有监督的训练数据特别少。另一方面，可以被命名的单词种类所受限制太少，所以从少量的数据样本中进行泛化很困难。所以为了解决上述问题，一般都是构建正交特征和类似地名索引（gazetteers）语言特定的知识语料。但是在新语言和新领域中开发基于语言特定知识的语料成本很高，这样使NER的挑战性更高。</p>
<p>无监督的未标注词典提供了一种可替代的策略，可以从小的样本数据中获得更好的泛化性能。然而，即使是在大多数都依赖于无监督特征的NER系统中，也是选择在此特征基础上增加手工标注的特征和特定的语言知识语料而不是替换掉无监督特征。</p>
<p>本文提出一种不使用语言特定的语料或特征，基于少量的有监督训练数据和未标注的词典实现一种NER网络架构。</p>
<p>本模型主要用于捕捉两个信息。</p>
<p>（1）第一点因为names总是含有多个token，所以对每个token作出合理的标注决策是很重要的。关于这一点，我们对比了两个模型。</p>
<p>（i）BLSTM+CRF。<br>（ii）S-LSTN。基于转换方法的构建和标注输入语句。</p>
<p>（2）对于token水平的判断名称的证据有两个，一个是正交（orthographic）证据，另一个是分布式（distributional）证据。</p>
<p>利用基于字向量的单词表示模型（character-based word representa- tion model）捕捉正交敏感特性；将分布式表示方法与其他表示方法结合起来来捕捉分布式敏感特性。</p>
<p>在英语、荷兰语、德语和西班牙语的实验上，表明基于LSTM-CRF的模型性能最好。基于转换的方法transition-based方法在很多语言模型上也超过了已发表的结果，虽然没有超过lstm-crf模型。</p>
<h1 id="2-LSTM-CRF-model"><a href="#2-LSTM-CRF-model" class="headerlink" title="2 LSTM-CRF model"></a>2 LSTM-CRF model</h1><p>简单介绍下LSTM和CRF，以及现在的混合标注架构。</p>
<h2 id="2-1-LSTM"><a href="#2-1-LSTM" class="headerlink" title="2.1 LSTM"></a>2.1 LSTM</h2><p>循环神经网络是对序列数据进行操作的一中神经网络。RNN将向量的序列$$(x_{1},x_{2},…x)<em>{n})$$作为输入，并得到一个输出序列$$h</em>{1},h_{2},…h_{n}$$,输出序列表示的是关于输入序列在每一时间步的信息。lstm是为了解决RNN长期依赖问题所提出的一种RNN的变体，lstm加入了一个记忆单元能够捕捉到长期的依赖信息。同时加入了门控单元，用于控制输入信息的哪部份将被送入记忆单元，历史信息的哪部份将被遗忘。</p>
<p>下图为门控单元的数学原理。</p>
<p><img src="lstm.png" alt=""></p>
<p>$$\sigma$$ 是sigmoid函数。</p>
<p>$$(x_{1},x_{2},…x)<em>{n})$$代表n个words，每个用d维的向量表示。利用双向lstm计算得出句子中第t个word左侧的信息$$\overrightarrow{h</em>{t}}$$,以及它右侧信息$$\overleftarrow{h_{t}}$$。</p>
<p>此模型将左右侧的输出并接起来得到最终的word representation，$$h_{t} = [\overrightarrow{h_{t}};\overleftarrow{h_{t}}]$$,这样这个word representation有效包含了该词的上下文信息，这一点在很多标注应用很有用处。</p>
<h2 id="2-2-CRF-Tagging-Models"><a href="#2-2-CRF-Tagging-Models" class="headerlink" title="2.2 CRF Tagging Models"></a>2.2 CRF Tagging Models</h2><p>一个非常简单但是具有很有效的标注模型是将隐藏层的输出 $$h_{t}$$作为特征，对每个输出向量的标注做独立的决策。尽管这种模型在POS标注的简单问题上比较成功，但是当输出标记间存在很强依赖性时，模型的独立分类决策受限。<br>NER是这样一种任务，因为字符序列间的语法被强加了集中约束，比如I-PER不会在B_LOC之后，所以这种独立假设模型不能应用在NER问题上。</p>
<p>所以，对于NER我们不采用独立标注决策模型，相反。采用条件随机场进行联合建模。</p>
<p>输入语句：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X = (x_&#123;1&#125;,x_&#123;2&#125;, ···x_&#123;n&#125;)</span><br></pre></td></tr></table></figure></p>
<p>假设P为双向LSTM网络的输出概率矩阵，size是<code>$n \times k$</code>,其中k代表标注数量，<code>$P_{i,j}$</code>表示输入语句的第i个单词的标注为第j个tag，预测序列为 </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Y = (y_&#123;1&#125;,y_&#123;2&#125;, ···y_&#123;n&#125;)</span><br></pre></td></tr></table></figure>
<p>将分数定义为</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">s(X,Y) = \sum _&#123;i=0&#125;^&#123;n&#125;A_&#123;y_&#123;i&#125;,y_&#123;i+1&#125;&#125; + \sum_&#123;i=1&#125;^&#123;n&#125;P_&#123;i,y_&#123;i&#125;&#125;</span><br></pre></td></tr></table></figure>
<p>其中，A代表转移概率矩阵，<code>$A_{i,j}$</code>代表tag从i转换到j的得分。<br><code>$y_{0}$</code>和<code>$y_{n}$</code>是认为添加的句子tag的始末。所以A为k+2的方阵。</p>
<p>在所有tag序列中，根据softmax函数得到序列y的tag概率。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">p(y|X) =\frac &#123;e^&#123;s(X,y)&#125;&#125; &#123;\sum_&#123;\hat&#123;y&#125;\in Y_&#123;X&#125;&#125;e^&#123;s(X,\hat&#123;y&#125;)&#125;&#125;</span><br></pre></td></tr></table></figure>
<p>在训练过程中，将正确tag的log概率逐渐最大化：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">log(p(y|X)) = s(X,y) - log(\sum_&#123;\hat&#123;y&#125;\in&#123;Y_&#123;X&#125;&#125;&#125; e^&#123;s(X,\hat&#123;y&#125;)&#125;) </span><br><span class="line"></span><br><span class="line">            =  s(X,y) - log add _&#123;\hat&#123;y&#125;\in&#123;Y_&#123;X&#125;&#125;&#125; s(X,\hat&#123;y&#125;)</span><br></pre></td></tr></table></figure>
<p>其中 $$ Y_{X}$$代表对于X序列所有可能的tag序列。上述公式表明模型将产生输出label的有效序列。在解码过程中，通过$$y^{*} = argmax <em>{\hat{y} \in {Y</em>{X}}} s(X,\hat{y})$$预测输出序列含有最高得分score。</p>
<h2 id="2-3-Parameterization-and-Training"><a href="#2-3-Parameterization-and-Training" class="headerlink" title="2.3  Parameterization and Training"></a>2.3  Parameterization and Training</h2><p>每个token的标注决策得分是由上下文的词（word-in-context）经过Blstm计算得出embedding值，然后embedding相互点乘得到这些得分再与bigram的兼容性得分相组合。</p>
<p>网络架构图如下，圆圈代表观察变量，双圆圈代表随机变量，菱形代表上一次层的决策函数。<br><img src="main_architecture.png" alt=""></p>
<p>模型参数包括：bigram兼容性得分矩阵A，Blstm概率矩阵，线性的特征权重以及wordembeddings。在2.2章节，假设<code>$x_{i}$</code>句子中单词的word embedding，<code>$y_{i}$</code>为该词相关联的tag。在section 4 将会继续讨论如何对word embedding进行建模。2.1章节也已经讲述了embedding输入到Blstm，网络输出该词左右的上下文表示（ a representation of the left and right context）。</p>
<p>将得到的上下文表示concatenate成向量<code>$c_{i}$</code>,然后将其部署在一个layer上大小与单个的tag大小相同。这一层不使用softmax输出，而是用CRF来选择输出每个单词tag的最后预测结果。另外，在<code>$c_{i}$</code>与CRF层之间添加一个隐藏层，能使性能大幅提高。</p>
<h2 id="2-4-Tagging-Schemes"><a href="#2-4-Tagging-Schemes" class="headerlink" title="2.4 Tagging Schemes"></a>2.4 Tagging Schemes</h2><p>使用IOBES，Inside，Outside，Beginning，End，Singel。</p>
<h1 id="3-Transition-Based-Chunking-Model"><a href="#3-Transition-Based-Chunking-Model" class="headerlink" title="3  Transition-Based Chunking Model"></a>3  Transition-Based Chunking Model</h1><p>序列型lstm是将序列从左到右进行建模，而栈式lstm允许对stack对象进行embedding，并可以从embedding中增加（push）或者移除（pop），使得stack-LSTM能够像stack一样工作，只包含上下文的主要的embedding。</p>
<h2 id="3-1-Chunking-Algorithm"><a href="#3-1-Chunking-Algorithm" class="headerlink" title="3.1 Chunking Algorithm"></a>3.1 Chunking Algorithm</h2><p>设计如下的过度库存（transition inventory ）。使用两个stack，分别是特定的输出和stack的表示，一个缓存buffer存储要被处理的单词。transition inventory 包含以下的transition，SHIFT是将word从buffer到stack，REDUCE（y）从stack的top将所有的items全都pop出去，创建一个chunk，并将这些items标记为y,最后将这些标记好的chunk reprentation push到output stack的top。当REDUCE成立时，OUT将word直接从buffer移动到output stack，<br>当buffer和stack都为空时，算法结束。</p>
<p><img src="chunking.png" alt=""></p>
<p><img src="slstm-example.png" alt=""></p>
<p>此模型的参数化是在给定当前stack、buffer、output的内容，以及历史action的前提下，对每一时间步的action定义一个概率分布。对上述的每一个通过stack lstm计算得出固定维度的embedding，然后通过并接embedding得到全部的算法状态。这种representation是用于在每个时间步对所有可能的操作定义一个分布。这个模型的目的是在给定输入序列的前提下，最大化该序列的参考操作（从标记的训练词典中提取得到）的条件概率。</p>
<p>采用贪婪算法得到最后的最大概率操作，对于长度为n的序列，所有操作的次数为2n次。</p>
<h2 id="3-2-Representing-Labeled-Chunks"><a href="#3-2-Representing-Labeled-Chunks" class="headerlink" title="3.2 Representing Labeled Chunks"></a>3.2 Representing Labeled Chunks</h2><p>当REDUCE（y）操作被执行时，算法将把一个序列的tokens（包括token的向量embedding）作为一个单独的chunk，从stack移动到output buffer。通过双向lstm计算序列的embedding，Blstm建立在embedding之上，输入数据包括组成这个embedding的所有tokens和代表该chunk类型的token（比如，y）。</p>
<h1 id="4-Input-Word-Embeddings"><a href="#4-Input-Word-Embeddings" class="headerlink" title="4 Input Word Embeddings"></a>4 Input Word Embeddings</h1><p>本文两个模型的输入都是单独单词的向量表示。从有限的NER训练数据中得到词类型的单独表示是很困难的，因为有太多的参数要评估。因为很多语言都能利用正字法或者形态法证明它是一个name，所以每个词的表示对于它们的拼写（spelling）很是敏感。所以，运用一个模型，通过构成这个单词的每个字符的表示来构建这个单词的表示。</p>
<p>另一个想法是，在大型语料词典中，命名可能会出现各种个样的变化。针对这种情况，从对词序敏感的大型语料词典中学习得到embedding。为了避免模型过于依赖某一个表示（representation），加上一层dropout，事实证明dropout对于得到好的泛化性能至关重要。</p>
<h2 id="4-1-Character-based-models-of-words"><a href="#4-1-Character-based-models-of-words" class="headerlink" title="4.1 Character-based models of words"></a>4.1 Character-based models of words</h2><p>本论文最大的特点是，在训练中学习字粒度特征，而不是手工建立单词前缀和后缀信息的特征工程。<br>学习字粒度的embedding有利于学习特定任务或者特定领域中的表示（representation）。研究表明，这一做法在以下几个方面有利：（1）形态丰富的语言，（2）解决PO<br>S中的词性标注和语言模型中超出词典（out-of-vocabulary）问题或者依赖解析。</p>
<p><img src="word_embedding.png" alt=""></p>
<p>上图为由单词的字符生成单词embedding的过程。<br>过程如下：</p>
<p>（1）字符查找表是随机初始化的，包含了每个字符的embedding。通过一个双向lstm将word中的每个character相关联起来，正向lstm得到character embedding的正向序列，反向lstm得到反向序列。</p>
<p>（2）从双向lstm得到正向和反向的character embedding序列，拼接起来得到一个word embedding，这个embedding是字粒度（character-level）。</p>
<p>（3）最后将（2）中字粒度的word embedding与词粒度的embedding拼接起来得到最后的embedding。词粒度的embedding是通过查找word lookup-tabel得到。</p>
<p>在测试中，将look-up table中没有embedding的单词映射为UNK，将50%的单个字也映射为UNK。双向lstm的正反向都是25维度，所以字粒度的embedding维度是50.</p>
<h2 id="4-2-Pretrained-embeddings"><a href="#4-2-Pretrained-embeddings" class="headerlink" title="4.2 Pretrained embeddings"></a>4.2 Pretrained embeddings</h2><p>实验表明，使用预训练的embedding效果要优于随机初始化的embedding。采用skip-n-gram的方法预训练embedding，是一种word2vec的变体。然后这些预训练的embedding在训练过程中不断调整。</p>
<p>embedding的维度为：英语100维，荷兰，德语，西班牙语为64。单词的最小频率为4，窗口大小为8。</p>
<h2 id="4-3-Dropout-training"><a href="#4-3-Dropout-training" class="headerlink" title="4.3 Dropout training"></a>4.3 Dropout training</h2><p>初始实验表明，当与词粒度的embedding并接后，字粒度的embedding并没有对性能发生改变。为了使模型能够充分利用这两种表示，加入了dropout层。在最后的embedding后，在输入到lstm模型前加dropout层。<br>dropout给模型性能带来了很大改进。</p>
<h1 id="5-Experiments"><a href="#5-Experiments" class="headerlink" title="5 Experiments"></a>5 Experiments</h1><h2 id="5-1-Training"><a href="#5-1-Training" class="headerlink" title="5.1 Training"></a>5.1 Training</h2><p>两个模型都是使用反向传播算法更新参数。<br>尝试使用SGD，学习率设为0.01，梯度剪枝设为5.有很多SGD的改进算法，比如Adadelta和Adam。运用这些改进算法可以加快收敛，但是性能都没有不加剪枝操作的好。</p>
<p>lstm-crf模型只使用一层双向lstm，维度为100。改变维度并没有对性能有改进。dropout层dropout rate设为0.5，增大会使性能变差，变小会使运行时间变长。</p>
<p>stack-lstm 使用两层lstm，维度都是100。每个构成方法的action的维度都是16，输出的embedding维度是20。</p>
<h2 id="5-2-Data-Sets"><a href="#5-2-Data-Sets" class="headerlink" title="5.2  Data Sets"></a>5.2  Data Sets</h2><p>验证模型对于不用语言的泛化能力，应用了数据集 CoNLL-2002 和 CoNLL- 2003 datasets。除了对英语中的数字全部替换为0以外，没有对数据做任何预处理。</p>
<h2 id="5-3-Results"><a href="#5-3-Results" class="headerlink" title="5.3 Results"></a>5.3 Results</h2><p>state-the-art<br><img src="English_german.png" alt=""><br><img src="dutch_spanish.png" alt=""></p>
<h2 id="5-4-Network-architectures"><a href="#5-4-Network-architectures" class="headerlink" title="5.4 Network architectures"></a>5.4 Network architectures</h2><p>探究了CRF，character-level representation，pretraining embedding，dropout对于模型性能的影响，pretrainning embedding的正向影响最大。具体影响为下：</p>
<p><img src="impact.png" alt=""></p>
<h1 id="6-Related-Work"><a href="#6-Related-Work" class="headerlink" title="6 Related Work"></a>6 Related Work</h1><p>CoNLL-2002 : Carreras 利用几个合适深度决策树的组合得到best one；<br>CoNLL-2003 :  Florian 利用四个不同的分类器的组合得到best one<br>2009 ：Qi利用神经网络，在大量未标注语料词典中运用无监督学习得到best one<br>之后的神经网络模型有：</p>
<p>CNN+CRF</p>
<p>LSTM+CRF+hand-crafted spelling feature</p>
<p>线性链式CRF+L2正则+短语聚类特征</p>
<p>线性链式CRF+spelling特征和地名索引</p>
<h1 id="7-Conclusion"><a href="#7-Conclusion" class="headerlink" title="7 Conclusion"></a>7 Conclusion</h1><p>关键点：<br>（1）其他模型是对输出标记的依赖性进行建模，而不是应用简单的CRF或者transition-bsed算法去构建和标记成块的输入。</p>
<p>（2）应用pre-trained word embedding，以及character-based word embedding，更好的捕捉到了形态和正交的信息。</p>
<p>（3）dropout</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.xuman-amy.cn/2018/08/11/BP-and-BPTT/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Amy">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="大毛毛啊">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/08/11/BP-and-BPTT/" itemprop="url">BP-and-BPTT</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-08-11T17:33:26+08:00">
                2018-08-11
              </time>
            

            

            
          </span>

          

          
            
          

          
          
             <span id="/2018/08/11/BP-and-BPTT/" class="leancloud_visitors" data-flag-title="BP-and-BPTT">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">Visitors&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<p>本文来自 ancewer 的CSDN 博客 ，全文地址请点击：<a href="https://blog.csdn.net/ancewer/article/details/73279127?utm_source=copy" target="_blank" rel="noopener">https://blog.csdn.net/ancewer/article/details/73279127?utm_source=copy</a> </p>
<h2 id="1-5、BP和BPTT"><a href="#1-5、BP和BPTT" class="headerlink" title="1.5、BP和BPTT"></a>1.5、BP和BPTT</h2><p><a href="http://www.cnblogs.com/pinard/p/6509630.html" target="_blank" rel="noopener">BPTT参考文件</a></p>
<p>1、反向传播算法（Backpropagation）</p>
<ul>
<li>反向传播算法要解决的问题</li>
</ul>
<p>深层神经网络（Deep Neural Network，DNN)由输入层、多个隐藏层和输出层组成，任务分为分类和回归两大类别。如果我们使用深层神经网络做了一个预测任务，预测输出为$\tilde{y}$，真实的为y，这时候就需要定义一个损失函数来评价预测任务的性能，接着进行损失函数的迭代优化使其达到最小值，并得到此时的权重矩阵和偏置值。在神经网络中一般利用梯度下降法（Gradient Descent）迭代求解损失函数的最小值。在深层神经网络中使用梯度下降法迭代优化损失函数使其达到最小值的算法就称为反向传播算法（Back Propagation，BP）。</p>
<ul>
<li>反向传播算法的推导过程</li>
</ul>
<p>假设深层网络第L层的输出为$$ a_{L} $$:<br> $$ \begin{split}<br> a^{L} &amp;= \sigma(z^{L}) \<br> &amp;= \sigma (W^{L} \cdot a^{L-1}  + b^{L})<br>\end{split} $$<br>定义损失函数$$J(w,b,x,y)$$为：<br> $$\begin{split}<br>J(w,b,x,y) &amp;= \frac{1} {2} \parallel a_{L} - y \parallel _{2} ^{2} \<br>&amp;=  \frac{1} {2} \parallel  \sigma(z^{L})  - y \parallel _{2} ^{2}\<br>&amp;=  \frac{1} {2} \parallel  \sigma( W^{L} \cdot a^{L-1}  + b^{L} ) - y \parallel <em>{2} ^{2}\<br>\end{split}$$<br>注解：$a</em>{L}$为预测输出,$y$为实际值，二者具有相同的维度。$\parallel \cdot \parallel_{2}$ 代表二范数。<br>对损失函数运用梯度下降法迭代求最小值，分别求解对于权重矩阵$W^{L}$和偏置$b^{L}$的梯度。</p>
<p><strong>损失函数对权重矩阵的梯度：</strong><br>$$\begin{split}<br>\frac{\partial  J(w,b,x,y)}{\partial  W^{L}} &amp;=<br>                    \frac{\partial  J(w,b,x,y)}{\partial  a^{L}} \cdot<br>                     \frac{\partial  a^{L}}{\partial  z^{L}}  \cdot<br>                     \frac{\partial  z^{L}}{\partial  w^{L}}   \<br>     &amp;= (a^{L} - y) \bigodot  \sigma^{‘}(z^{L}) \ast(a^{(L-1)})^{T}<br> \end{split}$$</p>
<p> <strong>损失函数对偏置的梯度：</strong><br>$$\begin{split}<br>\frac{\partial  J(w,b,x,y)}{\partial  b^{L}} &amp;=<br>                    \frac{\partial  J(w,b,x,y)}{\partial  a^{L}} \cdot<br>                     \frac{\partial  a^{L}}{\partial  z^{L}}  \cdot<br>                     \frac{\partial  z^{L}}{\partial  b^{L}}   \<br>     &amp;= (a^{L} - y) \bigodot  \sigma^{‘}(z^{L})<br> \end{split}$$</p>
<p> 其中公式中的符号$ \bigodot$ 代表Hadamard积，即维度相同的两个矩阵中位置相同的对应数相乘后的矩阵。</p>
<p> 损失函数对于权重矩阵和偏置的梯度含有共同项$$\frac{\partial  J(w,b,x,y)}{\partial  a^{L}} \cdot  \frac{\partial  a^{L}}{\partial  z^{L}} $$，令其等于$$\delta^{L}$$。</p>
<p> 可以求得$$  \delta^{L}$$为<br>  $$\begin{split}<br>  \delta^{L} &amp;= \frac{\partial  J(w,b,x,y)}{\partial  a^{L}} \cdot  \frac{\partial  a^{L}}{\partial  z^{L}}  \<br>&amp;= (a^{L} - y) \bigodot  \sigma^{‘}(z^{L})<br> \end{split}$$</p>
<p> 知道L层的$$  \delta^{L}$$就可以利用数学归纳法递归的求出L-1，L-2……各层的梯度。<br>   $$\begin{split}<br>  \delta^{l} &amp;= \frac{\partial  J(w,b,x,y)}{\partial  a^{l}} \cdot  \frac{\partial  a^{l}}{\partial  z^{l}} \<br>  &amp;= \frac{\partial  J(w,b,x,y)}{\partial  a^{L}} \cdot<br>          \frac{\partial  a^{L}}{\partial  z^{L}} \cdot<br>          \frac{\partial  z^{L}}{\partial  z^{L-l}} \cdot<br>          \frac{\partial  z^{L-1}}{\partial  z^{L-2}}  ……<br>          \cdot \frac{\partial  z^{l+1}}{\partial  z^{l}}<br> \end{split}$$<br> 又知：<br>$$ z^{l} = W^{l} \cdot a^{l-1}  + b^{l} $$</p>
<p>所以第$$l$$层的梯度$$W^{l}、b^{l}$$可以表示为 ：<br> $$\begin{split}<br>\frac{\partial  J(w,b,x,y)}{\partial  W^{l}} &amp;=  \delta^{l} (a^{(l-1)})^{T}\<br> \frac{\partial  J(w,b,x,y)}{\partial  b^{l}} &amp;=  \delta^{l}<br> \end{split}$$</p>
<p>数学归纳法求：<br>  $$\begin{split}<br> \delta^{l}  &amp;=  \frac{\partial  J(w,b,x,y)}{\partial  a^{l}} \cdot<br>                          \frac{\partial  a^{l}}{\partial  z^{l}} \<br>                 &amp; =  \frac{\partial  J(w,b,x,y)}{\partial  a^{l+1}} \cdot<br>                          \frac{\partial  a^{l+1}}{\partial  z^{l+1}} \cdot<br>                          \frac{\partial  z^{l+1}}{\partial  z^{l}} \<br>                 &amp; =  \delta^{l+1}  \frac{\partial  z^{l+1}}{\partial  z^{l}}<br>\end{split}$$<br>又知：<br>$$\begin{split}<br>z^{l+1} &amp;= W^{l+1} \cdot a^{l}  + b^{l+1} \<br>    &amp;= W^{l+1} \cdot  \sigma( z^{l})+ b^{l+1}<br>\end{split}$$<br>所以可得：<br>$$\begin{split}<br>\frac{\partial  z^{l+1}}{\partial  z^{l}} &amp;= ( W^{l+1})^{T} \bigodot  \sigma ^{‘}( z^{l})<br>\end{split}$$<br>可得：<br>  $$\begin{split}<br> \delta^{l}  &amp; =  \delta^{l+1}  \frac{\partial  z^{l+1}}{\partial  z^{l}}  \<br>                 &amp;= \delta^{l+1} ( W^{l+1})^{T}\bigodot  \sigma ^{‘}( z^{l})<br>\end{split}$$</p>
<p>求得了$ \delta^{l}$ 的递推关系之后，就可以依次求得各层的梯度$W^{l}和b^{l}$了。</p>
<p>2、 随时间的反向传播过程（Back Propagation Through Time）</p>
<p>循环神经网络的特点是利用上下文做出对当前时刻的预测，RNN的循环也正是随时间进行的，采用梯度下降法对循环神经网络的损失函数进行迭代优化求其最小值时也是随时间进行的，所以这个也被称为随时间的反向传播（Back Propagation Through Time，BPTT），区别于深层神经网络中的反向传播（BP）。</p>
<p><img src="RNN_15.png" alt="RNN 逻辑图"></p>
<ul>
<li>为了更易被读者理解推导过程，如上图所示，我们进行以下定义：  </li>
<li>U：输入层的权重矩阵  </li>
<li>W：隐藏层的权重矩阵  </li>
<li>V：输出层的权重矩阵  </li>
<li>t时刻的输入为$$x^{(t)}$$：同理$$x^{(t+1)}$$为t+1时刻的输入信息。  </li>
<li>t时刻的隐藏层状态为$$h^{(t)}$$：由$$x^{(t)}$$和$$h^{(t-1)}$$共同决定。<br>$$\begin{split}<br>h^{t} &amp;=\sigma(z^{(t)}) \<br>&amp;= \sigma (U \cdot x^{(t)} + W \cdot h^{(t-1)} + b)\<br>&amp;=tanh (U \cdot x^{(t)} + W \cdot h^{(t-1)} + b)<br>\end{split}$$</li>
<li>t时刻的输出为$$o^{(t)}$$：只由t时刻的隐藏状态$h^{(t)}$决定。<br>$$ o^{t} = V \cdot h^{(t)} +c $$  </li>
<li>真实的值为$y^{t}$，预测的值为$\hat{y}$。<br>$$\begin{split}<br>\hat{y} &amp;= \sigma(o^{(t)})  \<br>&amp;= softmax(o^{(t)})<br>\end{split}$$</li>
<li><p>t 时刻的损失函数为$L^{t}$，评价预测的性能也就是量化预测值与真实值之间的差，本篇假设损失函数为交叉熵<br>$$L^{(t)} = y^{(t)} log( \hat{y^{(t)}} ) + ( 1 - y^{(t)}) log(1- \hat{y^{(t)}} ) $$<br>最终的损失函数为各个时刻损失函数的加和,即<br>$$ L = \sum_{t=1}^{\tau} L^{(t)}$$</p>
</li>
<li><p>注解：</p>
</li>
</ul>
<p>(1) U，V，W为线性共享参数，在循环神经网络的不同时刻不同位置，这三个权重矩阵的值是一样的，这也正是RNN的循环所在。<br>(2) 假设损失函数为交叉熵，也就是等价于对数损失函数，隐藏层激活函数为tanh函数，输出层激活函数为softmax函数。</p>
<ul>
<li>BPTT的推导</li>
</ul>
<p>因为我们假设的输出层激活函数为softmax函数，所以得到以下公式：<br>$$\begin{split}<br>\frac{\partial{L^{(t)}} }{\partial{o^{(t)}}}  =  \sum_{t=1}^{t=\tau} (\hat{ y^{(t)}}-y^{(t)})<br>\end{split}$$</p>
<p>假设隐藏层的函数为tanh函数，可得<br>$$\begin{split}<br>\frac{\partial{h^{(t)}} }{\partial{h^{(t-1)}}}  &amp;= W^{T} diag(1-h^{(t)} \bigodot h^{(t)})\<br>\frac{\partial{h^{(t)}} }{\partial{U}}  &amp;= (x^{(t)})^{T} diag(1-h^{(t)} \bigodot h^{(t)})\<br>\frac{\partial{h^{(t)}} }{\partial{W}}  &amp;= (h^{(t-1)})^{T} diag(1-h^{(t)} \bigodot h^{(t)})\<br>\frac{\partial{h^{(t)}} }{\partial{b}}  &amp;= diag(1-h^{(t)} \bigodot h^{(t)})\<br>\end{split}$$<br><strong>损失函数对于c的梯度：</strong><br>$$\begin{split}<br>\frac{\partial{L(U,V, W,b, c)}}{\partial{c}}  &amp;= \sum_{t=1}^{t=\tau}\frac{\partial{L^{(t)}} }{\partial{o^{(t)}}} \frac{\partial{o^{(t)}} }{\partial{c}} \<br>&amp;= \sum_{t=1}^{t=\tau}  y^{(t)}-y^{(t)}<br>\end{split}$$<br><strong>损失函数对于V的梯度：</strong><br>$$\begin{split}<br>\frac{\partial{L(U,V, W,b, c)}}{\partial{V}}  &amp;= \sum_{t=1}^{t=\tau}\frac{\partial{L^{(t)}} }{\partial{o^{(t)}}} \frac{\partial{o^{(t)}} }{\partial{V}} \<br>&amp;= \sum_{t=1}^{t=\tau}  (\hat{y^{(t)}}-y^{(t)}) (h^{(t)})^{T}<br>\end{split}$$<br><strong>损失函数对于W, U, b的梯度：</strong><br>随时间的反向传播算法中，循环神经网络的梯度损失由当前时间步t的梯度和下一时刻t+1的梯度损失两部分决定。<br>定义损失函数对于隐藏状态$$h^{(t)}$$的梯度为：<br>$$\delta ^{(t)} = \frac{\partial{L(U,V, W,b, c)}}{\partial{h^{(t)}}}$$<br>类似于前文所说的深层神经网络中的反向传播算法，可以由$$\delta ^{(t+1)}$$递推出$$\delta ^{(t)}$$，公式如下：</p>
<p>$$\begin{split}<br>\delta ^{(t)} &amp;=<br>\frac{\partial{L^{(t)}} }{\partial{o^{(t)}}} \frac{\partial{o^{(t)}} }{\partial{h^{(t)}}} +<br>\frac{\partial{L^{(t)}} }{\partial{h^{(t+1)}}} \frac{\partial{h^{(t+1)}} }{\partial{h^{(t)}}}  \<br>&amp;= V^{T}(\hat{y^{(t)}}-y^{(t)}) +W^{T}\delta ^{(t+1)} diag(1 - h^{(t+<br>1)} \bigodot h^{(t+1)})<br>\end{split}$$</p>
<p>注意：<br>对于$$\delta ^{(\tau)}$$因为没有下一时刻的信息了，所以<br>$$\begin{split}<br>\delta ^{(\tau)} &amp;=<br>\frac{\partial{L^{(\tau}} }{\partial{o^{(\tau)}}} \frac{\partial{o^{(\tau)}} }{\partial{h^{(\tau)}}}\<br>&amp;= V^{T}(\hat{y^{(\tau)}}-y^{(\tau)})<br>\end{split}$$</p>
<p>在递推出了以上公式后，计算损失函数对于W，U，b的梯度就比较简单了。<br>$$\begin{split}<br>\frac{\partial{L(U,V, W,b, c)}}{\partial{U}}  &amp;= \sum_{t=1}^{t=\tau}\frac{\partial{L^{(t)}} }{\partial{h^{(t)}}} \frac{\partial{h^{(t)}} }{\partial{U}} \<br>&amp;= \sum_{t=1}^{t=\tau}  \delta^{(t)} (x^{(t)})^{T} diag(1-h^{(t)} \bigodot h^{(t)})\<br>\end{split}$$</p>
<p>$$\begin{split}<br>\frac{\partial{L(U,V, W,b, c)}}{\partial{W}}  &amp;= \sum_{t=1}^{t=\tau}\frac{\partial{L^{(t)}} }{\partial{h^{(t)}}} \frac{\partial{h^{(t)}} }{\partial{W}} \<br>&amp;= \sum_{t=1}^{t=\tau}  \delta^{(t)} (h^{(t-1)})^{T} diag(1-h^{(t)} \bigodot h^{(t)})\<br>\end{split}$$</p>
<p>$$\begin{split}<br>\frac{\partial{L(U,V, W,b, c)}}{\partial{b}}  &amp;= \sum_{t=1}^{t=\tau}\frac{\partial{L^{(t)}} }{\partial{h^{(t)}}} \frac{\partial{h^{(t)}} }{\partial{b}} \<br>&amp;= \sum_{t=1}^{t=\tau}  \delta^{(t)} diag(1-h^{(t)} \bigodot h^{(t)})\<br>\end{split}$$</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  
    
  

  <article class="post post-type-normal post-sticky" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.xuman-amy.cn/2018/08/11/The-first-blog-0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Amy">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="大毛毛啊">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                  <span class="post-sticky-flag" title="Sticky">
                    <i class="fa fa-thumb-tack"></i>
                  </span>
                
                <a class="post-title-link" href="/2018/08/11/The-first-blog-0/" itemprop="url"> The First Blog</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-08-11T16:57:44+08:00">
                2018-08-11
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/cate/" itemprop="url" rel="index">
                    <span itemprop="name">cate</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          
             <span id="/2018/08/11/The-first-blog-0/" class="leancloud_visitors" data-flag-title=" The First Blog">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">Visitors&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>无论如何，学着热爱这一场有去无回的人生。</p>
<p>无论多少岁，希望我们都能享受自由。</p>
<p><img src="top.png" alt=""></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  


          </div>
          


          
        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Amy</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">8</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">3</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">5</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/xuman-Amy" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://blog.csdn.net/Amy_mm/" target="_blank" title="CSDN">
                      
                        <i class="fa fa-fw fa-CSDN"></i>CSDN</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:buptamy@gmail.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Amy</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  







<!-- LOCAL: You can save these files to your site and update links -->
    
        
        <link rel="stylesheet" href="https://aimingoo.github.io/gitmint/style/default.css">
        <script src="https://aimingoo.github.io/gitmint/dist/gitmint.browser.js"></script>
    
<!-- END LOCAL -->

    

    







  





  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script>
  <script>AV.initialize("<app_id>", "");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  

  
  

  

  

  


  
  <script type="text/javascript" src="//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js"></script>
  
</body>
</html>
