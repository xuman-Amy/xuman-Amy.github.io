<!DOCTYPE html>



  

<html class="theme-next pisces use-motion" lang="">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="paper notes," />










<meta name="description" content="《Neural Architectures for Named Entity Recognition》摘要为了从有监督的比较小的训练词典中有效学习，最好的命名实体识别系统（named entity recognition）的实现很大部分依赖于手工标注的特征（features）和领域知识（domain-specific knowledge）。 本论文介绍两个新的网络架构，一个是基于双向lstm和条件">
<meta name="keywords" content="paper notes">
<meta property="og:type" content="article">
<meta property="og:title" content="《Neural Architectures for Named Entity Recognition》 paper notes">
<meta property="og:url" content="http://www.xuman-amy.cn/2018/08/13/Neural-Architectures-for-Named-Entity-Recognition/index.html">
<meta property="og:site_name" content="大毛毛啊">
<meta property="og:description" content="《Neural Architectures for Named Entity Recognition》摘要为了从有监督的比较小的训练词典中有效学习，最好的命名实体识别系统（named entity recognition）的实现很大部分依赖于手工标注的特征（features）和领域知识（domain-specific knowledge）。 本论文介绍两个新的网络架构，一个是基于双向lstm和条件">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://www.xuman-amy.cn/2018/08/13/Neural-Architectures-for-Named-Entity-Recognition/lstm.png">
<meta property="og:image" content="http://www.xuman-amy.cn/2018/08/13/Neural-Architectures-for-Named-Entity-Recognition/main_architecture.png">
<meta property="og:image" content="http://www.xuman-amy.cn/2018/08/13/Neural-Architectures-for-Named-Entity-Recognition/chunking.png">
<meta property="og:image" content="http://www.xuman-amy.cn/2018/08/13/Neural-Architectures-for-Named-Entity-Recognition/slstm-example.png">
<meta property="og:image" content="http://www.xuman-amy.cn/2018/08/13/Neural-Architectures-for-Named-Entity-Recognition/word_embedding.png">
<meta property="og:image" content="http://www.xuman-amy.cn/2018/08/13/Neural-Architectures-for-Named-Entity-Recognition/English_german.png">
<meta property="og:image" content="http://www.xuman-amy.cn/2018/08/13/Neural-Architectures-for-Named-Entity-Recognition/dutch_spanish.png">
<meta property="og:image" content="http://www.xuman-amy.cn/2018/08/13/Neural-Architectures-for-Named-Entity-Recognition/impact.png">
<meta property="og:updated_time" content="2018-09-21T09:38:35.303Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="《Neural Architectures for Named Entity Recognition》 paper notes">
<meta name="twitter:description" content="《Neural Architectures for Named Entity Recognition》摘要为了从有监督的比较小的训练词典中有效学习，最好的命名实体识别系统（named entity recognition）的实现很大部分依赖于手工标注的特征（features）和领域知识（domain-specific knowledge）。 本论文介绍两个新的网络架构，一个是基于双向lstm和条件">
<meta name="twitter:image" content="http://www.xuman-amy.cn/2018/08/13/Neural-Architectures-for-Named-Entity-Recognition/lstm.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"right","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://www.xuman-amy.cn/2018/08/13/Neural-Architectures-for-Named-Entity-Recognition/"/>





  <title>《Neural Architectures for Named Entity Recognition》 paper notes | 大毛毛啊</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">
<a href="https://github.com/xuman-Amy"><img style="position: absolute; top: 0; left: 0; border: 0;" src="https://s3.amazonaws.com/github/ribbons/forkme_left_darkblue_121621.png" alt="Fork me on GitHub"></a>

  
  
    
  

  <div class="container sidebar-position-right page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">大毛毛啊</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.xuman-amy.cn/2018/08/13/Neural-Architectures-for-Named-Entity-Recognition/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Amy">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="大毛毛啊">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">《Neural Architectures for Named Entity Recognition》 paper notes</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-08-13T00:00:00+08:00">
                2018-08-13
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NER/" itemprop="url" rel="index">
                    <span itemprop="name">NER</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/08/13/Neural-Architectures-for-Named-Entity-Recognition/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count gitment-comments-count" data-xid="/2018/08/13/Neural-Architectures-for-Named-Entity-Recognition/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          
          
             <span id="/2018/08/13/Neural-Architectures-for-Named-Entity-Recognition/" class="leancloud_visitors" data-flag-title="《Neural Architectures for Named Entity Recognition》 paper notes">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">Visitors&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="《Neural-Architectures-for-Named-Entity-Recognition》"><a href="#《Neural-Architectures-for-Named-Entity-Recognition》" class="headerlink" title="《Neural Architectures for Named Entity Recognition》"></a>《Neural Architectures for Named Entity Recognition》</h1><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>为了从有监督的比较小的训练词典中有效学习，最好的命名实体识别系统（named entity recognition）的实现很大部分依赖于手工标注的特征（features）和领域知识（domain-specific knowledge）。</p>
<p>本论文介绍两个新的网络架构，一个是基于双向lstm和条件岁机场，另一个是受shift-reduce解析器的启发，基于转换的方法进行构建和标注片段。</p>
<p>本文模型依赖于词信息的两个信息源，一个是来源于有监督词典的基于字符的单词表示；另一个是基于无监督的未标注的词表示。</p>
<p>在没有借助任何语言相关的知识和类似地名索引（gazetteers）资源的前提下，我们的模型在四种语言的NER中得到了最好的性能。</p>
<h1 id="1-引言"><a href="#1-引言" class="headerlink" title="1 引言"></a>1 引言</h1><p>NER是一个很有挑战性的学习问题。一方面因为在大多数的语言和领域中，可利用的有监督的训练数据特别少。另一方面，可以被命名的单词种类所受限制太少，所以从少量的数据样本中进行泛化很困难。所以为了解决上述问题，一般都是构建正交特征和类似地名索引（gazetteers）语言特定的知识语料。但是在新语言和新领域中开发基于语言特定知识的语料成本很高，这样使NER的挑战性更高。</p>
<p>无监督的未标注词典提供了一种可替代的策略，可以从小的样本数据中获得更好的泛化性能。然而，即使是在大多数都依赖于无监督特征的NER系统中，也是选择在此特征基础上增加手工标注的特征和特定的语言知识语料而不是替换掉无监督特征。</p>
<p>本文提出一种不使用语言特定的语料或特征，基于少量的有监督训练数据和未标注的词典实现一种NER网络架构。</p>
<p>本模型主要用于捕捉两个信息。</p>
<p>（1）第一点因为names总是含有多个token，所以对每个token作出合理的标注决策是很重要的。关于这一点，我们对比了两个模型。</p>
<p>（i）BLSTM+CRF。<br>（ii）S-LSTN。基于转换方法的构建和标注输入语句。</p>
<p>（2）对于token水平的判断名称的证据有两个，一个是正交（orthographic）证据，另一个是分布式（distributional）证据。</p>
<p>利用基于字向量的单词表示模型（character-based word representa- tion model）捕捉正交敏感特性；将分布式表示方法与其他表示方法结合起来来捕捉分布式敏感特性。</p>
<p>在英语、荷兰语、德语和西班牙语的实验上，表明基于LSTM-CRF的模型性能最好。基于转换的方法transition-based方法在很多语言模型上也超过了已发表的结果，虽然没有超过lstm-crf模型。</p>
<h1 id="2-LSTM-CRF-model"><a href="#2-LSTM-CRF-model" class="headerlink" title="2 LSTM-CRF model"></a>2 LSTM-CRF model</h1><p>简单介绍下LSTM和CRF，以及现在的混合标注架构。</p>
<h2 id="2-1-LSTM"><a href="#2-1-LSTM" class="headerlink" title="2.1 LSTM"></a>2.1 LSTM</h2><p>循环神经网络是对序列数据进行操作的一中神经网络。RNN将向量的序列$$(x_{1},x_{2},…x)<em>{n})$$作为输入，并得到一个输出序列$$h</em>{1},h_{2},…h_{n}$$,输出序列表示的是关于输入序列在每一时间步的信息。lstm是为了解决RNN长期依赖问题所提出的一种RNN的变体，lstm加入了一个记忆单元能够捕捉到长期的依赖信息。同时加入了门控单元，用于控制输入信息的哪部份将被送入记忆单元，历史信息的哪部份将被遗忘。</p>
<p>下图为门控单元的数学原理。</p>
<p><img src="lstm.png" alt=""></p>
<p>$$\sigma$$ 是sigmoid函数。</p>
<p>$$(x_{1},x_{2},…x)<em>{n})$$代表n个words，每个用d维的向量表示。利用双向lstm计算得出句子中第t个word左侧的信息$$\overrightarrow{h</em>{t}}$$,以及它右侧信息$$\overleftarrow{h_{t}}$$。</p>
<p>此模型将左右侧的输出并接起来得到最终的word representation，$$h_{t} = [\overrightarrow{h_{t}};\overleftarrow{h_{t}}]$$,这样这个word representation有效包含了该词的上下文信息，这一点在很多标注应用很有用处。</p>
<h2 id="2-2-CRF-Tagging-Models"><a href="#2-2-CRF-Tagging-Models" class="headerlink" title="2.2 CRF Tagging Models"></a>2.2 CRF Tagging Models</h2><p>一个非常简单但是具有很有效的标注模型是将隐藏层的输出 $$h_{t}$$作为特征，对每个输出向量的标注做独立的决策。尽管这种模型在POS标注的简单问题上比较成功，但是当输出标记间存在很强依赖性时，模型的独立分类决策受限。<br>NER是这样一种任务，因为字符序列间的语法被强加了集中约束，比如I-PER不会在B_LOC之后，所以这种独立假设模型不能应用在NER问题上。</p>
<p>所以，对于NER我们不采用独立标注决策模型，相反。采用条件随机场进行联合建模。</p>
<p>输入语句：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X = (x_&#123;1&#125;,x_&#123;2&#125;, ···x_&#123;n&#125;)</span><br></pre></td></tr></table></figure></p>
<p>假设P为双向LSTM网络的输出概率矩阵，size是<code>$n \times k$</code>,其中k代表标注数量，<code>$P_{i,j}$</code>表示输入语句的第i个单词的标注为第j个tag，预测序列为 </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Y = (y_&#123;1&#125;,y_&#123;2&#125;, ···y_&#123;n&#125;)</span><br></pre></td></tr></table></figure>
<p>将分数定义为</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">s(X,Y) = \sum _&#123;i=0&#125;^&#123;n&#125;A_&#123;y_&#123;i&#125;,y_&#123;i+1&#125;&#125; + \sum_&#123;i=1&#125;^&#123;n&#125;P_&#123;i,y_&#123;i&#125;&#125;</span><br></pre></td></tr></table></figure>
<p>其中，A代表转移概率矩阵，<code>$A_{i,j}$</code>代表tag从i转换到j的得分。<br><code>$y_{0}$</code>和<code>$y_{n}$</code>是认为添加的句子tag的始末。所以A为k+2的方阵。</p>
<p>在所有tag序列中，根据softmax函数得到序列y的tag概率。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">p(y|X) =\frac &#123;e^&#123;s(X,y)&#125;&#125; &#123;\sum_&#123;\hat&#123;y&#125;\in Y_&#123;X&#125;&#125;e^&#123;s(X,\hat&#123;y&#125;)&#125;&#125;</span><br></pre></td></tr></table></figure>
<p>在训练过程中，将正确tag的log概率逐渐最大化：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">log(p(y|X)) = s(X,y) - log(\sum_&#123;\hat&#123;y&#125;\in&#123;Y_&#123;X&#125;&#125;&#125; e^&#123;s(X,\hat&#123;y&#125;)&#125;) </span><br><span class="line"></span><br><span class="line">            =  s(X,y) - log add _&#123;\hat&#123;y&#125;\in&#123;Y_&#123;X&#125;&#125;&#125; s(X,\hat&#123;y&#125;)</span><br></pre></td></tr></table></figure>
<p>其中 $$ Y_{X}$$代表对于X序列所有可能的tag序列。上述公式表明模型将产生输出label的有效序列。在解码过程中，通过$$y^{*} = argmax <em>{\hat{y} \in {Y</em>{X}}} s(X,\hat{y})$$预测输出序列含有最高得分score。</p>
<h2 id="2-3-Parameterization-and-Training"><a href="#2-3-Parameterization-and-Training" class="headerlink" title="2.3  Parameterization and Training"></a>2.3  Parameterization and Training</h2><p>每个token的标注决策得分是由上下文的词（word-in-context）经过Blstm计算得出embedding值，然后embedding相互点乘得到这些得分再与bigram的兼容性得分相组合。</p>
<p>网络架构图如下，圆圈代表观察变量，双圆圈代表随机变量，菱形代表上一次层的决策函数。<br><img src="main_architecture.png" alt=""></p>
<p>模型参数包括：bigram兼容性得分矩阵A，Blstm概率矩阵，线性的特征权重以及wordembeddings。在2.2章节，假设<code>$x_{i}$</code>句子中单词的word embedding，<code>$y_{i}$</code>为该词相关联的tag。在section 4 将会继续讨论如何对word embedding进行建模。2.1章节也已经讲述了embedding输入到Blstm，网络输出该词左右的上下文表示（ a representation of the left and right context）。</p>
<p>将得到的上下文表示concatenate成向量<code>$c_{i}$</code>,然后将其部署在一个layer上大小与单个的tag大小相同。这一层不使用softmax输出，而是用CRF来选择输出每个单词tag的最后预测结果。另外，在<code>$c_{i}$</code>与CRF层之间添加一个隐藏层，能使性能大幅提高。</p>
<h2 id="2-4-Tagging-Schemes"><a href="#2-4-Tagging-Schemes" class="headerlink" title="2.4 Tagging Schemes"></a>2.4 Tagging Schemes</h2><p>使用IOBES，Inside，Outside，Beginning，End，Singel。</p>
<h1 id="3-Transition-Based-Chunking-Model"><a href="#3-Transition-Based-Chunking-Model" class="headerlink" title="3  Transition-Based Chunking Model"></a>3  Transition-Based Chunking Model</h1><p>序列型lstm是将序列从左到右进行建模，而栈式lstm允许对stack对象进行embedding，并可以从embedding中增加（push）或者移除（pop），使得stack-LSTM能够像stack一样工作，只包含上下文的主要的embedding。</p>
<h2 id="3-1-Chunking-Algorithm"><a href="#3-1-Chunking-Algorithm" class="headerlink" title="3.1 Chunking Algorithm"></a>3.1 Chunking Algorithm</h2><p>设计如下的过度库存（transition inventory ）。使用两个stack，分别是特定的输出和stack的表示，一个缓存buffer存储要被处理的单词。transition inventory 包含以下的transition，SHIFT是将word从buffer到stack，REDUCE（y）从stack的top将所有的items全都pop出去，创建一个chunk，并将这些items标记为y,最后将这些标记好的chunk reprentation push到output stack的top。当REDUCE成立时，OUT将word直接从buffer移动到output stack，<br>当buffer和stack都为空时，算法结束。</p>
<p><img src="chunking.png" alt=""></p>
<p><img src="slstm-example.png" alt=""></p>
<p>此模型的参数化是在给定当前stack、buffer、output的内容，以及历史action的前提下，对每一时间步的action定义一个概率分布。对上述的每一个通过stack lstm计算得出固定维度的embedding，然后通过并接embedding得到全部的算法状态。这种representation是用于在每个时间步对所有可能的操作定义一个分布。这个模型的目的是在给定输入序列的前提下，最大化该序列的参考操作（从标记的训练词典中提取得到）的条件概率。</p>
<p>采用贪婪算法得到最后的最大概率操作，对于长度为n的序列，所有操作的次数为2n次。</p>
<h2 id="3-2-Representing-Labeled-Chunks"><a href="#3-2-Representing-Labeled-Chunks" class="headerlink" title="3.2 Representing Labeled Chunks"></a>3.2 Representing Labeled Chunks</h2><p>当REDUCE（y）操作被执行时，算法将把一个序列的tokens（包括token的向量embedding）作为一个单独的chunk，从stack移动到output buffer。通过双向lstm计算序列的embedding，Blstm建立在embedding之上，输入数据包括组成这个embedding的所有tokens和代表该chunk类型的token（比如，y）。</p>
<h1 id="4-Input-Word-Embeddings"><a href="#4-Input-Word-Embeddings" class="headerlink" title="4 Input Word Embeddings"></a>4 Input Word Embeddings</h1><p>本文两个模型的输入都是单独单词的向量表示。从有限的NER训练数据中得到词类型的单独表示是很困难的，因为有太多的参数要评估。因为很多语言都能利用正字法或者形态法证明它是一个name，所以每个词的表示对于它们的拼写（spelling）很是敏感。所以，运用一个模型，通过构成这个单词的每个字符的表示来构建这个单词的表示。</p>
<p>另一个想法是，在大型语料词典中，命名可能会出现各种个样的变化。针对这种情况，从对词序敏感的大型语料词典中学习得到embedding。为了避免模型过于依赖某一个表示（representation），加上一层dropout，事实证明dropout对于得到好的泛化性能至关重要。</p>
<h2 id="4-1-Character-based-models-of-words"><a href="#4-1-Character-based-models-of-words" class="headerlink" title="4.1 Character-based models of words"></a>4.1 Character-based models of words</h2><p>本论文最大的特点是，在训练中学习字粒度特征，而不是手工建立单词前缀和后缀信息的特征工程。<br>学习字粒度的embedding有利于学习特定任务或者特定领域中的表示（representation）。研究表明，这一做法在以下几个方面有利：（1）形态丰富的语言，（2）解决PO<br>S中的词性标注和语言模型中超出词典（out-of-vocabulary）问题或者依赖解析。</p>
<p><img src="word_embedding.png" alt=""></p>
<p>上图为由单词的字符生成单词embedding的过程。<br>过程如下：</p>
<p>（1）字符查找表是随机初始化的，包含了每个字符的embedding。通过一个双向lstm将word中的每个character相关联起来，正向lstm得到character embedding的正向序列，反向lstm得到反向序列。</p>
<p>（2）从双向lstm得到正向和反向的character embedding序列，拼接起来得到一个word embedding，这个embedding是字粒度（character-level）。</p>
<p>（3）最后将（2）中字粒度的word embedding与词粒度的embedding拼接起来得到最后的embedding。词粒度的embedding是通过查找word lookup-tabel得到。</p>
<p>在测试中，将look-up table中没有embedding的单词映射为UNK，将50%的单个字也映射为UNK。双向lstm的正反向都是25维度，所以字粒度的embedding维度是50.</p>
<h2 id="4-2-Pretrained-embeddings"><a href="#4-2-Pretrained-embeddings" class="headerlink" title="4.2 Pretrained embeddings"></a>4.2 Pretrained embeddings</h2><p>实验表明，使用预训练的embedding效果要优于随机初始化的embedding。采用skip-n-gram的方法预训练embedding，是一种word2vec的变体。然后这些预训练的embedding在训练过程中不断调整。</p>
<p>embedding的维度为：英语100维，荷兰，德语，西班牙语为64。单词的最小频率为4，窗口大小为8。</p>
<h2 id="4-3-Dropout-training"><a href="#4-3-Dropout-training" class="headerlink" title="4.3 Dropout training"></a>4.3 Dropout training</h2><p>初始实验表明，当与词粒度的embedding并接后，字粒度的embedding并没有对性能发生改变。为了使模型能够充分利用这两种表示，加入了dropout层。在最后的embedding后，在输入到lstm模型前加dropout层。<br>dropout给模型性能带来了很大改进。</p>
<h1 id="5-Experiments"><a href="#5-Experiments" class="headerlink" title="5 Experiments"></a>5 Experiments</h1><h2 id="5-1-Training"><a href="#5-1-Training" class="headerlink" title="5.1 Training"></a>5.1 Training</h2><p>两个模型都是使用反向传播算法更新参数。<br>尝试使用SGD，学习率设为0.01，梯度剪枝设为5.有很多SGD的改进算法，比如Adadelta和Adam。运用这些改进算法可以加快收敛，但是性能都没有不加剪枝操作的好。</p>
<p>lstm-crf模型只使用一层双向lstm，维度为100。改变维度并没有对性能有改进。dropout层dropout rate设为0.5，增大会使性能变差，变小会使运行时间变长。</p>
<p>stack-lstm 使用两层lstm，维度都是100。每个构成方法的action的维度都是16，输出的embedding维度是20。</p>
<h2 id="5-2-Data-Sets"><a href="#5-2-Data-Sets" class="headerlink" title="5.2  Data Sets"></a>5.2  Data Sets</h2><p>验证模型对于不用语言的泛化能力，应用了数据集 CoNLL-2002 和 CoNLL- 2003 datasets。除了对英语中的数字全部替换为0以外，没有对数据做任何预处理。</p>
<h2 id="5-3-Results"><a href="#5-3-Results" class="headerlink" title="5.3 Results"></a>5.3 Results</h2><p>state-the-art<br><img src="English_german.png" alt=""><br><img src="dutch_spanish.png" alt=""></p>
<h2 id="5-4-Network-architectures"><a href="#5-4-Network-architectures" class="headerlink" title="5.4 Network architectures"></a>5.4 Network architectures</h2><p>探究了CRF，character-level representation，pretraining embedding，dropout对于模型性能的影响，pretrainning embedding的正向影响最大。具体影响为下：</p>
<p><img src="impact.png" alt=""></p>
<h1 id="6-Related-Work"><a href="#6-Related-Work" class="headerlink" title="6 Related Work"></a>6 Related Work</h1><p>CoNLL-2002 : Carreras 利用几个合适深度决策树的组合得到best one；<br>CoNLL-2003 :  Florian 利用四个不同的分类器的组合得到best one<br>2009 ：Qi利用神经网络，在大量未标注语料词典中运用无监督学习得到best one<br>之后的神经网络模型有：</p>
<p>CNN+CRF</p>
<p>LSTM+CRF+hand-crafted spelling feature</p>
<p>线性链式CRF+L2正则+短语聚类特征</p>
<p>线性链式CRF+spelling特征和地名索引</p>
<h1 id="7-Conclusion"><a href="#7-Conclusion" class="headerlink" title="7 Conclusion"></a>7 Conclusion</h1><p>关键点：<br>（1）其他模型是对输出标记的依赖性进行建模，而不是应用简单的CRF或者transition-bsed算法去构建和标记成块的输入。</p>
<p>（2）应用pre-trained word embedding，以及character-based word embedding，更好的捕捉到了形态和正交的信息。</p>
<p>（3）dropout</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/paper-notes/" rel="tag"># paper notes</a>
          
        </div>
      

      
      
        <div class="post-widgets">
        
          <div class="wp_rating">
            <div id="wpac-rating"></div>
          </div>
        

        

        
        </div>
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/08/11/BP-and-BPTT/" rel="next" title="BP-and-BPTT">
                <i class="fa fa-chevron-left"></i> BP-and-BPTT
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/08/23/test/" rel="prev" title="test">
                test <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      
        <div id="gitment-container"></div>
      
    </div>

  

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Amy</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">6</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">2</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">3</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/xuman-Amy" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://blog.csdn.net/Amy_mm/" target="_blank" title="CSDN">
                      
                        <i class="fa fa-fw fa-CSDN"></i>CSDN</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:buptamy@gmail.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#《Neural-Architectures-for-Named-Entity-Recognition》"><span class="nav-number">1.</span> <span class="nav-text">《Neural Architectures for Named Entity Recognition》</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#摘要"><span class="nav-number">2.</span> <span class="nav-text">摘要</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#1-引言"><span class="nav-number">3.</span> <span class="nav-text">1 引言</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2-LSTM-CRF-model"><span class="nav-number">4.</span> <span class="nav-text">2 LSTM-CRF model</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#2-1-LSTM"><span class="nav-number">4.1.</span> <span class="nav-text">2.1 LSTM</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-2-CRF-Tagging-Models"><span class="nav-number">4.2.</span> <span class="nav-text">2.2 CRF Tagging Models</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-3-Parameterization-and-Training"><span class="nav-number">4.3.</span> <span class="nav-text">2.3  Parameterization and Training</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-4-Tagging-Schemes"><span class="nav-number">4.4.</span> <span class="nav-text">2.4 Tagging Schemes</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#3-Transition-Based-Chunking-Model"><span class="nav-number">5.</span> <span class="nav-text">3  Transition-Based Chunking Model</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#3-1-Chunking-Algorithm"><span class="nav-number">5.1.</span> <span class="nav-text">3.1 Chunking Algorithm</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-2-Representing-Labeled-Chunks"><span class="nav-number">5.2.</span> <span class="nav-text">3.2 Representing Labeled Chunks</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#4-Input-Word-Embeddings"><span class="nav-number">6.</span> <span class="nav-text">4 Input Word Embeddings</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#4-1-Character-based-models-of-words"><span class="nav-number">6.1.</span> <span class="nav-text">4.1 Character-based models of words</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-2-Pretrained-embeddings"><span class="nav-number">6.2.</span> <span class="nav-text">4.2 Pretrained embeddings</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-3-Dropout-training"><span class="nav-number">6.3.</span> <span class="nav-text">4.3 Dropout training</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#5-Experiments"><span class="nav-number">7.</span> <span class="nav-text">5 Experiments</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#5-1-Training"><span class="nav-number">7.1.</span> <span class="nav-text">5.1 Training</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-2-Data-Sets"><span class="nav-number">7.2.</span> <span class="nav-text">5.2  Data Sets</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-3-Results"><span class="nav-number">7.3.</span> <span class="nav-text">5.3 Results</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-4-Network-architectures"><span class="nav-number">7.4.</span> <span class="nav-text">5.4 Network architectures</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#6-Related-Work"><span class="nav-number">8.</span> <span class="nav-text">6 Related Work</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#7-Conclusion"><span class="nav-number">9.</span> <span class="nav-text">7 Conclusion</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Amy</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  







<!-- LOCAL: You can save these files to your site and update links -->
    
        
        <link rel="stylesheet" href="https://aimingoo.github.io/gitmint/style/default.css">
        <script src="https://aimingoo.github.io/gitmint/dist/gitmint.browser.js"></script>
    
<!-- END LOCAL -->

    

    
      <script type="text/javascript">
      function renderGitment(){
        var gitment = new Gitmint({
            id: window.location.pathname, 
            owner: 'xuman-Amy# MUST HAVE, Your Github ID',
            repo: 'xuman-Amy.github.io# MUST HAVE, The repo you use to store Gitment comments',
            
            lang: "" || navigator.language || navigator.systemLanguage || navigator.userLanguage,
            
            oauth: {
            
            
                client_secret: 'c43680f61789011096eb106066420686cc354729# EITHER this or proxy_gateway, Github access secret token for the Gitment',
            
                client_id: '0b3ec68f8515185e02d2# MUST HAVE, Github client id for the Gitment'
            }});
        gitment.render('gitment-container');
      }

      
      renderGitment();
      
      </script>
    







  





  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script>
  <script>AV.initialize("<app_id>", "");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  

  
  
  <script type="text/javascript">
  wpac_init = window.wpac_init || [];
  wpac_init.push({widget: 'Rating', id: <app_id>,
    el: 'wpac-rating',
    color: 'fc6423'
  });
  (function() {
    if ('WIDGETPACK_LOADED' in window) return;
    WIDGETPACK_LOADED = true;
    var mc = document.createElement('script');
    mc.type = 'text/javascript';
    mc.async = true;
    mc.src = '//embed.widgetpack.com/widget.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(mc, s.nextSibling);
  })();
  </script>


  

  

  

</body>
</html>
